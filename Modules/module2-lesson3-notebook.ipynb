{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2, Lesson 3: Data Sources, APIs, and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Data Sources\n",
    "**Internal vs External Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Internal Data Sources\n",
    "Internal data comes from within your organization. This example simulates different internal systems that a company might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating internal company data sources\n",
    "\n",
    "# Source 1: Sales Database\n",
    "sales_data = pd.DataFrame({\n",
    "    'transaction_id': range(1000, 1010),\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'product_id': np.random.choice(['P001', 'P002', 'P003'], 10),\n",
    "    'quantity': np.random.randint(1, 5, 10),\n",
    "    'revenue': np.random.uniform(100, 1000, 10).round(2)\n",
    "})\n",
    "\n",
    "# Source 2: HR System\n",
    "hr_data = pd.DataFrame({\n",
    "    'employee_id': ['E001', 'E002', 'E003', 'E004', 'E005'],\n",
    "    'name': ['Alice Smith', 'Bob Jones', 'Carol White', 'David Brown', 'Eve Davis'],\n",
    "    'department': ['Sales', 'IT', 'Sales', 'HR', 'IT'],\n",
    "    'hire_date': pd.to_datetime(['2020-01-15', '2019-06-01', '2021-03-20', '2018-11-10', '2022-02-01']),\n",
    "    'salary': [65000, 85000, 62000, 70000, 90000]\n",
    "})\n",
    "\n",
    "# Source 3: Customer CRM\n",
    "crm_data = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C002', 'C003', 'C004'],\n",
    "    'company': ['TechCorp', 'RetailMax', 'FinanceHub', 'HealthPlus'],\n",
    "    'contact_email': ['contact@techcorp.com', 'info@retailmax.com', 'hello@financehub.com', 'support@healthplus.com'],\n",
    "    'account_status': ['Active', 'Active', 'Pending', 'Inactive'],\n",
    "    'last_contact': pd.to_datetime(['2024-01-10', '2024-01-08', '2024-01-05', '2023-12-15'])\n",
    "})\n",
    "\n",
    "print(\"INTERNAL DATA SOURCES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. Sales Database (Transactional System):\")\n",
    "print(sales_data.head())\n",
    "print(f\"\\nShape: {sales_data.shape}\")\n",
    "\n",
    "print(\"\\n2. HR System (Employee Data):\")\n",
    "print(hr_data)\n",
    "print(f\"\\nShape: {hr_data.shape}\")\n",
    "\n",
    "print(\"\\n3. CRM System (Customer Data):\")\n",
    "print(crm_data)\n",
    "print(f\"\\nShape: {crm_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: External Data Sources\n",
    "External data comes from outside your organization. These are public or purchased datasets that provide additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating external data sources\n",
    "\n",
    "# External Source 1: Government Economic Data\n",
    "economic_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=6, freq='M'),\n",
    "    'unemployment_rate': [3.7, 3.8, 3.6, 3.5, 3.6, 3.7],\n",
    "    'inflation_rate': [2.1, 2.2, 2.0, 2.3, 2.4, 2.2],\n",
    "    'gdp_growth': [2.5, 2.4, 2.6, 2.7, 2.5, 2.8]\n",
    "})\n",
    "\n",
    "# External Source 2: Industry Benchmarks\n",
    "benchmark_data = pd.DataFrame({\n",
    "    'metric': ['Customer Acquisition Cost', 'Churn Rate', 'Average Order Value', 'Conversion Rate'],\n",
    "    'industry_average': [150, 0.05, 225, 0.03],\n",
    "    'top_performer': [100, 0.02, 350, 0.05],\n",
    "    'our_company': [175, 0.04, 245, 0.035]\n",
    "})\n",
    "\n",
    "# External Source 3: Social Media Sentiment\n",
    "social_data = pd.DataFrame({\n",
    "    'week': pd.date_range('2024-01-01', periods=4, freq='W'),\n",
    "    'mentions': [234, 189, 267, 312],\n",
    "    'positive_sentiment': [0.65, 0.72, 0.58, 0.70],\n",
    "    'negative_sentiment': [0.20, 0.15, 0.25, 0.18],\n",
    "    'neutral_sentiment': [0.15, 0.13, 0.17, 0.12]\n",
    "})\n",
    "\n",
    "print(\"EXTERNAL DATA SOURCES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. Government Economic Data:\")\n",
    "print(economic_data)\n",
    "\n",
    "print(\"\\n2. Industry Benchmarks:\")\n",
    "print(benchmark_data)\n",
    "\n",
    "print(\"\\n3. Social Media Analytics:\")\n",
    "print(social_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Working with Different File Formats\n",
    "**Common Data File Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: CSV Files\n",
    "CSV is the most universal format. It's human-readable but doesn't preserve data types or handle special characters well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=5),\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Cable'],\n",
    "    'price': [999.99, 29.99, 79.99, 299.99, 9.99],\n",
    "    'quantity': [2, 10, 5, 3, 20],\n",
    "    'in_stock': [True, True, False, True, True]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "sample_data.to_csv('sample_data.csv', index=False)\n",
    "print(\"Original data types:\")\n",
    "print(sample_data.dtypes)\n",
    "print(\"\\nData preview:\")\n",
    "print(sample_data)\n",
    "\n",
    "# Read back from CSV\n",
    "csv_data = pd.read_csv('sample_data.csv')\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"After reading from CSV:\")\n",
    "print(csv_data.dtypes)\n",
    "print(\"\\nNotice: Date became string, boolean became bool (lucky!)\")\n",
    "\n",
    "# Proper way to read CSV with correct types\n",
    "csv_data_correct = pd.read_csv('sample_data.csv', parse_dates=['date'])\n",
    "print(\"\\nWith parse_dates parameter:\")\n",
    "print(csv_data_correct.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: JSON Files\n",
    "JSON is perfect for nested/hierarchical data and APIs. It preserves structure but can be verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested JSON data\n",
    "json_data = {\n",
    "    \"company\": \"DataCorp\",\n",
    "    \"founded\": 2020,\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Alice Johnson\",\n",
    "            \"role\": \"Data Scientist\",\n",
    "            \"skills\": [\"Python\", \"SQL\", \"Machine Learning\"],\n",
    "            \"projects\": {\n",
    "                \"current\": \"Customer Segmentation\",\n",
    "                \"completed\": 5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Bob Smith\",\n",
    "            \"role\": \"Data Engineer\",\n",
    "            \"skills\": [\"SQL\", \"Spark\", \"AWS\"],\n",
    "            \"projects\": {\n",
    "                \"current\": \"Data Pipeline\",\n",
    "                \"completed\": 8\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"metrics\": {\n",
    "        \"revenue\": 1500000,\n",
    "        \"growth_rate\": 0.25,\n",
    "        \"client_count\": 42\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open('company_data.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(\"JSON Structure:\")\n",
    "print(json.dumps(json_data, indent=2))\n",
    "\n",
    "# Read and parse JSON\n",
    "with open('company_data.json', 'r') as f:\n",
    "    loaded_json = json.load(f)\n",
    "\n",
    "# Extract nested data into DataFrame\n",
    "employees_df = pd.DataFrame(loaded_json['employees'])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Employees extracted to DataFrame:\")\n",
    "print(employees_df)\n",
    "\n",
    "# Normalize nested JSON to flat table\n",
    "employees_normalized = pd.json_normalize(loaded_json['employees'])\n",
    "print(\"\\nNormalized (flattened) employee data:\")\n",
    "print(employees_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Excel Files\n",
    "Excel files can have multiple sheets, formatting, and formulas. Great for business users but slower to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-sheet Excel file\n",
    "with pd.ExcelWriter('company_report.xlsx', engine='openpyxl') as writer:\n",
    "    # Sheet 1: Sales data\n",
    "    sales = pd.DataFrame({\n",
    "        'Month': pd.date_range('2024-01-01', periods=6, freq='M'),\n",
    "        'Revenue': [45000, 52000, 48000, 61000, 58000, 65000],\n",
    "        'Costs': [30000, 35000, 32000, 38000, 36000, 40000]\n",
    "    })\n",
    "    sales['Profit'] = sales['Revenue'] - sales['Costs']\n",
    "    sales.to_excel(writer, sheet_name='Sales', index=False)\n",
    "    \n",
    "    # Sheet 2: Employee data\n",
    "    employees = pd.DataFrame({\n",
    "        'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Department': ['Sales', 'IT', 'Finance'],\n",
    "        'Performance': [92, 88, 95]\n",
    "    })\n",
    "    employees.to_excel(writer, sheet_name='Employees', index=False)\n",
    "    \n",
    "    # Sheet 3: Summary\n",
    "    summary = pd.DataFrame({\n",
    "        'Metric': ['Total Revenue', 'Total Costs', 'Total Profit', 'Avg Performance'],\n",
    "        'Value': [sales['Revenue'].sum(), sales['Costs'].sum(), \n",
    "                 sales['Profit'].sum(), employees['Performance'].mean()]\n",
    "    })\n",
    "    summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "print(\"Excel file created with multiple sheets\")\n",
    "\n",
    "# Read Excel file - all sheets\n",
    "all_sheets = pd.read_excel('company_report.xlsx', sheet_name=None)\n",
    "print(\"\\nSheets in Excel file:\", list(all_sheets.keys()))\n",
    "\n",
    "# Read specific sheet\n",
    "sales_data = pd.read_excel('company_report.xlsx', sheet_name='Sales')\n",
    "print(\"\\nSales Sheet:\")\n",
    "print(sales_data)\n",
    "\n",
    "# Read multiple sheets at once\n",
    "print(\"\\nSummary Sheet:\")\n",
    "print(all_sheets['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Working with APIs\n",
    "**Getting Data from Web Services**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Understanding API Responses\n",
    "APIs return structured data (usually JSON) that we can parse and analyze. This simulates a typical API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an API response (what you'd get from a real API)\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"timestamp\": \"2024-01-15T10:30:00Z\",\n",
    "    \"data\": {\n",
    "        \"user_stats\": {\n",
    "            \"total_users\": 15234,\n",
    "            \"active_users\": 8921,\n",
    "            \"new_users_today\": 127\n",
    "        },\n",
    "        \"recent_activity\": [\n",
    "            {\"user_id\": \"U001\", \"action\": \"login\", \"time\": \"10:25:00\"},\n",
    "            {\"user_id\": \"U002\", \"action\": \"purchase\", \"time\": \"10:26:00\"},\n",
    "            {\"user_id\": \"U003\", \"action\": \"view_product\", \"time\": \"10:27:00\"},\n",
    "            {\"user_id\": \"U004\", \"action\": \"logout\", \"time\": \"10:28:00\"},\n",
    "            {\"user_id\": \"U005\", \"action\": \"add_to_cart\", \"time\": \"10:29:00\"}\n",
    "        ],\n",
    "        \"performance_metrics\": {\n",
    "            \"response_time_ms\": 245,\n",
    "            \"uptime_percentage\": 99.95,\n",
    "            \"error_rate\": 0.002\n",
    "        }\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"page\": 1,\n",
    "        \"per_page\": 5,\n",
    "        \"total_pages\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"RAW API RESPONSE:\")\n",
    "print(json.dumps(api_response, indent=2))\n",
    "\n",
    "# Extract specific data\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXTRACTED DATA:\")\n",
    "print(f\"\\nStatus: {api_response['status']}\")\n",
    "print(f\"Total Users: {api_response['data']['user_stats']['total_users']:,}\")\n",
    "print(f\"Active Users: {api_response['data']['user_stats']['active_users']:,}\")\n",
    "\n",
    "# Convert activity to DataFrame\n",
    "activity_df = pd.DataFrame(api_response['data']['recent_activity'])\n",
    "print(\"\\nRecent Activity (as DataFrame):\")\n",
    "print(activity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Real API Call (Public API)\n",
    "Let's make a real API call to a public service that doesn't require authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a public API that doesn't require authentication\n",
    "# JSONPlaceholder - Fake REST API for testing\n",
    "\n",
    "try:\n",
    "    # Make API request\n",
    "    response = requests.get('https://jsonplaceholder.typicode.com/posts?userId=1')\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        posts = response.json()\n",
    "        \n",
    "        print(f\"Successfully retrieved {len(posts)} posts\")\n",
    "        print(\"\\nFirst post (raw JSON):\")\n",
    "        print(json.dumps(posts[0], indent=2))\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        posts_df = pd.DataFrame(posts)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Posts as DataFrame:\")\n",
    "        print(posts_df[['id', 'title']].head())\n",
    "        \n",
    "        # Basic analysis\n",
    "        print(\"\\nPost Statistics:\")\n",
    "        print(f\"Total posts: {len(posts_df)}\")\n",
    "        print(f\"Avg title length: {posts_df['title'].str.len().mean():.1f} characters\")\n",
    "        print(f\"Avg body length: {posts_df['body'].str.len().mean():.1f} characters\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"API request failed with status code: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error making API request: {e}\")\n",
    "    print(\"This is normal if you're offline or the API is down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: API Best Practices\n",
    "When working with APIs, you need to handle errors, rate limits, and authentication properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_api_call(url, max_retries=3, timeout=5):\n",
    "    \"\"\"\n",
    "    Make an API call with proper error handling and retries.\n",
    "    This is a template for production API calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Make request with timeout\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            \n",
    "            # Check status code\n",
    "            if response.status_code == 200:\n",
    "                return {'success': True, 'data': response.json()}\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limited\n",
    "                return {'success': False, 'error': 'Rate limited. Try again later.'}\n",
    "            elif response.status_code == 404:\n",
    "                return {'success': False, 'error': 'Resource not found'}\n",
    "            else:\n",
    "                return {'success': False, 'error': f'HTTP {response.status_code}'}\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt == max_retries - 1:\n",
    "                return {'success': False, 'error': 'Request timed out'}\n",
    "            continue\n",
    "            \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            return {'success': False, 'error': 'Connection failed'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    return {'success': False, 'error': 'Max retries exceeded'}\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing safe API call function:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Good URL\n",
    "result = safe_api_call('https://jsonplaceholder.typicode.com/posts/1')\n",
    "if result['success']:\n",
    "    print(\"‚úì Successful call:\")\n",
    "    print(f\"  Retrieved post with title: {result['data']['title'][:50]}...\")\n",
    "else:\n",
    "    print(f\"‚úó Failed: {result['error']}\")\n",
    "\n",
    "# Bad URL (404)\n",
    "result = safe_api_call('https://jsonplaceholder.typicode.com/posts/99999')\n",
    "print(f\"\\n404 Test: {result}\")\n",
    "\n",
    "# Timeout test (unreachable URL)\n",
    "result = safe_api_call('https://10.255.255.1', timeout=1)\n",
    "print(f\"\\nTimeout Test: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Data Quality Assessment\n",
    "**Identifying and Measuring Data Quality Issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9: Creating Messy Data\n",
    "Let's create a dataset with common quality issues to learn how to identify and fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with various quality issues\n",
    "np.random.seed(42)\n",
    "\n",
    "messy_data = pd.DataFrame({\n",
    "    'order_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010,\n",
    "                 1002, 1012, 1013, 1014, 1015],  # Duplicate 1002\n",
    "    'customer_name': ['John Smith', 'jane doe', 'Bob JONES', '  Alice Brown  ', 'Charlie Wilson',\n",
    "                     'Diana Prince', 'Edward Norton', None, 'Fiona Green', 'George Harris',\n",
    "                     'Jane Doe', 'Helen Troy', 'Ian Malcolm', '', 'Kate Johnson'],  # Inconsistent, missing\n",
    "    'email': ['john@email.com', 'jane@email', 'bob@email.com', 'alice@email.com', 'NOT PROVIDED',\n",
    "             'diana@email.com', 'edward@', None, 'fiona@email.com', 'george@email.com',\n",
    "             'jane@email.com', 'helen@email.com', 'ian@invalid', 'kate@email.com', 'kate@email.com'],  # Invalid emails\n",
    "    'order_amount': ['100.50', '200', 150.75, '99.99', -50,  # Mixed types, negative value\n",
    "                    300, '0', 425.50, 180.25, 999999,  # Outlier\n",
    "                    200.00, 175.50, None, 225.75, 150.00],  # Missing value\n",
    "    'order_date': ['2024-01-15', '2024-01-16', '2024/01/17', '01-18-2024', '2024-01-19',\n",
    "                  '2024-01-20', '2024-01-21', '2024-01-22', '2025-12-31', '2024-01-24',  # Future date\n",
    "                  '2024-01-16', '2024-01-26', '2024-01-27', '1900-01-01', '2024-01-29'],  # Old date\n",
    "    'product_category': ['Electronics', 'electronics', 'ELECTRONICS', 'Clothing', 'clothing',\n",
    "                        'Food', 'food', 'Electronics', 'Books', 'books',\n",
    "                        'Electronics', 'Clothing', None, 'Unknown', 'Food']  # Inconsistent case\n",
    "})\n",
    "\n",
    "print(\"Dataset with Quality Issues:\")\n",
    "print(messy_data)\n",
    "print(\"\\nData types:\")\n",
    "print(messy_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10: Completeness Check\n",
    "Completeness measures how much of your required data is actually present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(df):\n",
    "    \"\"\"\n",
    "    Assess the completeness of each column in the dataset.\n",
    "    \"\"\"\n",
    "    print(\"COMPLETENESS ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Overall completeness\n",
    "    total_cells = df.size\n",
    "    missing_cells = df.isna().sum().sum()\n",
    "    # Count empty strings as missing too\n",
    "    empty_strings = (df == '').sum().sum()\n",
    "    total_missing = missing_cells + empty_strings\n",
    "    \n",
    "    completeness = ((total_cells - total_missing) / total_cells) * 100\n",
    "    \n",
    "    print(f\"Overall Completeness: {completeness:.1f}%\")\n",
    "    print(f\"Total cells: {total_cells}\")\n",
    "    print(f\"Missing values: {missing_cells}\")\n",
    "    print(f\"Empty strings: {empty_strings}\")\n",
    "    \n",
    "    print(\"\\nPer-Column Completeness:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        empty = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "        total_issues = missing + empty\n",
    "        pct_complete = ((len(df) - total_issues) / len(df)) * 100\n",
    "        \n",
    "        status = \"‚úì\" if pct_complete == 100 else \"‚úó\"\n",
    "        print(f\"{status} {col:20} {pct_complete:5.1f}% complete\")\n",
    "        if total_issues > 0:\n",
    "            print(f\"    ‚Üí {missing} missing, {empty} empty\")\n",
    "    \n",
    "    return completeness\n",
    "\n",
    "completeness_score = check_completeness(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11: Validity Check\n",
    "Validity checks whether data values are within acceptable ranges and formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(df):\n",
    "    \"\"\"\n",
    "    Check for invalid values in the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\nVALIDITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check email format\n",
    "    if 'email' in df.columns:\n",
    "        email_pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
    "        invalid_emails = df[~df['email'].str.match(email_pattern, na=False) & df['email'].notna()]\n",
    "        if len(invalid_emails) > 0:\n",
    "            print(f\"Invalid emails found: {len(invalid_emails)}\")\n",
    "            print(f\"  Examples: {invalid_emails['email'].head(3).tolist()}\")\n",
    "            issues.append(f\"{len(invalid_emails)} invalid emails\")\n",
    "    \n",
    "    # Check for negative amounts\n",
    "    if 'order_amount' in df.columns:\n",
    "        # Convert to numeric first\n",
    "        amounts = pd.to_numeric(df['order_amount'], errors='coerce')\n",
    "        negative_amounts = amounts[amounts < 0]\n",
    "        if len(negative_amounts) > 0:\n",
    "            print(f\"\\nNegative amounts found: {len(negative_amounts)}\")\n",
    "            print(f\"  Values: {negative_amounts.tolist()}\")\n",
    "            issues.append(f\"{len(negative_amounts)} negative amounts\")\n",
    "    \n",
    "    # Check for future dates\n",
    "    if 'order_date' in df.columns:\n",
    "        dates = pd.to_datetime(df['order_date'], errors='coerce')\n",
    "        future_dates = dates[dates > pd.Timestamp.now()]\n",
    "        if len(future_dates) > 0:\n",
    "            print(f\"\\nFuture dates found: {len(future_dates)}\")\n",
    "            print(f\"  Dates: {future_dates.dt.date.tolist()}\")\n",
    "            issues.append(f\"{len(future_dates)} future dates\")\n",
    "        \n",
    "        # Check for very old dates\n",
    "        old_dates = dates[dates < pd.Timestamp('2000-01-01')]\n",
    "        if len(old_dates) > 0:\n",
    "            print(f\"\\nSuspiciously old dates found: {len(old_dates)}\")\n",
    "            print(f\"  Dates: {old_dates.dt.date.tolist()}\")\n",
    "            issues.append(f\"{len(old_dates)} old dates\")\n",
    "    \n",
    "    validity_score = max(0, 100 - (len(issues) * 10))\n",
    "    print(f\"\\nValidity Score: {validity_score}%\")\n",
    "    print(f\"Total issues found: {len(issues)}\")\n",
    "    \n",
    "    return validity_score\n",
    "\n",
    "validity_score = check_validity(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 12: Consistency Check\n",
    "Consistency ensures that similar data is represented the same way throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistency(df):\n",
    "    \"\"\"\n",
    "    Check for inconsistent data representations.\n",
    "    \"\"\"\n",
    "    print(\"\\nCONSISTENCY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check text columns for inconsistent capitalization\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].notna().any():\n",
    "            # Get unique values when standardized to lowercase\n",
    "            original_unique = df[col].dropna().unique()\n",
    "            lowercase_unique = df[col].dropna().str.lower().unique()\n",
    "            \n",
    "            if len(original_unique) > len(lowercase_unique):\n",
    "                print(f\"\\nInconsistent capitalization in '{col}':\")\n",
    "                # Group by lowercase to show variations\n",
    "                for value in lowercase_unique[:3]:  # Show first 3 examples\n",
    "                    variations = df[df[col].str.lower() == value][col].unique()\n",
    "                    if len(variations) > 1:\n",
    "                        print(f\"  '{value}' appears as: {variations.tolist()}\")\n",
    "                issues.append(f\"Inconsistent capitalization in {col}\")\n",
    "    \n",
    "    # Check date formats\n",
    "    if 'order_date' in df.columns:\n",
    "        date_formats = []\n",
    "        for date_str in df['order_date'].dropna():\n",
    "            if '/' in str(date_str):\n",
    "                date_formats.append('YYYY/MM/DD')\n",
    "            elif str(date_str).count('-') == 2:\n",
    "                if str(date_str).index('-') == 4:\n",
    "                    date_formats.append('YYYY-MM-DD')\n",
    "                else:\n",
    "                    date_formats.append('MM-DD-YYYY')\n",
    "        \n",
    "        unique_formats = set(date_formats)\n",
    "        if len(unique_formats) > 1:\n",
    "            print(f\"\\nMultiple date formats detected: {unique_formats}\")\n",
    "            issues.append(\"Multiple date formats\")\n",
    "    \n",
    "    consistency_score = max(0, 100 - (len(issues) * 15))\n",
    "    print(f\"\\nConsistency Score: {consistency_score}%\")\n",
    "    \n",
    "    return consistency_score\n",
    "\n",
    "consistency_score = check_consistency(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 13: Uniqueness Check\n",
    "Uniqueness identifies duplicate records that shouldn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_uniqueness(df, key_column='order_id'):\n",
    "    \"\"\"\n",
    "    Check for duplicate records in the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\nUNIQUENESS ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check primary key uniqueness\n",
    "    if key_column in df.columns:\n",
    "        total_records = len(df)\n",
    "        unique_records = df[key_column].nunique()\n",
    "        duplicate_records = total_records - unique_records\n",
    "        \n",
    "        print(f\"Primary Key: {key_column}\")\n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Unique values: {unique_records}\")\n",
    "        print(f\"Duplicate records: {duplicate_records}\")\n",
    "        \n",
    "        if duplicate_records > 0:\n",
    "            # Show duplicates\n",
    "            duplicates = df[df.duplicated(subset=[key_column], keep=False)]\n",
    "            print(f\"\\nDuplicate {key_column} values:\")\n",
    "            duplicate_ids = duplicates[key_column].value_counts()\n",
    "            for id_val, count in duplicate_ids.items():\n",
    "                print(f\"  {id_val}: appears {count} times\")\n",
    "        \n",
    "        uniqueness_score = (unique_records / total_records) * 100\n",
    "    else:\n",
    "        print(f\"Key column '{key_column}' not found\")\n",
    "        uniqueness_score = 0\n",
    "    \n",
    "    # Check for complete row duplicates\n",
    "    complete_duplicates = df.duplicated().sum()\n",
    "    if complete_duplicates > 0:\n",
    "        print(f\"\\nComplete row duplicates: {complete_duplicates}\")\n",
    "    \n",
    "    print(f\"\\nUniqueness Score: {uniqueness_score:.1f}%\")\n",
    "    \n",
    "    return uniqueness_score\n",
    "\n",
    "uniqueness_score = check_uniqueness(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 14: Accuracy Check (Outliers)\n",
    "Accuracy involves identifying values that might be correct but are suspicious outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(df):\n",
    "    \"\"\"\n",
    "    Check for potential accuracy issues like outliers.\n",
    "    \"\"\"\n",
    "    print(\"\\nACCURACY ASSESSMENT (Outlier Detection)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check numeric columns for outliers\n",
    "    if 'order_amount' in df.columns:\n",
    "        amounts = pd.to_numeric(df['order_amount'], errors='coerce')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        Q1 = amounts.quantile(0.25)\n",
    "        Q3 = amounts.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Find outliers\n",
    "        outliers = amounts[(amounts < lower_bound) | (amounts > upper_bound)]\n",
    "        \n",
    "        print(f\"Order Amount Analysis:\")\n",
    "        print(f\"  Mean: ${amounts.mean():.2f}\")\n",
    "        print(f\"  Median: ${amounts.median():.2f}\")\n",
    "        print(f\"  Std Dev: ${amounts.std():.2f}\")\n",
    "        print(f\"\\n  IQR Method:\")\n",
    "        print(f\"    Q1 (25%): ${Q1:.2f}\")\n",
    "        print(f\"    Q3 (75%): ${Q3:.2f}\")\n",
    "        print(f\"    IQR: ${IQR:.2f}\")\n",
    "        print(f\"    Normal range: ${lower_bound:.2f} to ${upper_bound:.2f}\")\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            print(f\"\\n  Outliers detected: {len(outliers)}\")\n",
    "            print(f\"    Values: {outliers.tolist()}\")\n",
    "        else:\n",
    "            print(\"\\n  No outliers detected\")\n",
    "        \n",
    "        # Z-score method\n",
    "        z_scores = np.abs((amounts - amounts.mean()) / amounts.std())\n",
    "        z_outliers = amounts[z_scores > 3]\n",
    "        \n",
    "        print(f\"\\n  Z-Score Method (|z| > 3):\")\n",
    "        if len(z_outliers) > 0:\n",
    "            print(f\"    Extreme outliers: {len(z_outliers)}\")\n",
    "            print(f\"    Values: {z_outliers.tolist()}\")\n",
    "        else:\n",
    "            print(\"    No extreme outliers\")\n",
    "    \n",
    "    accuracy_score = 100 - (len(outliers) * 5) if 'outliers' in locals() else 100\n",
    "    accuracy_score = max(0, accuracy_score)\n",
    "    \n",
    "    print(f\"\\nAccuracy Score: {accuracy_score}%\")\n",
    "    \n",
    "    return accuracy_score\n",
    "\n",
    "accuracy_score = check_accuracy(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 15: Comprehensive Data Quality Report\n",
    "Combine all quality dimensions into a single report card for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\" \" * 20 + \"DATA QUALITY REPORT CARD\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Dataset: Order Data\")\n",
    "    print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "    print(f\"Records: {len(df)}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Run all checks\n",
    "    scores = {}\n",
    "    \n",
    "    # Quick versions of checks (without printing)\n",
    "    # Completeness\n",
    "    total_missing = df.isna().sum().sum() + (df == '').sum().sum()\n",
    "    scores['Completeness'] = ((df.size - total_missing) / df.size) * 100\n",
    "    \n",
    "    # Validity (simplified)\n",
    "    scores['Validity'] = validity_score  # Use previously calculated\n",
    "    \n",
    "    # Consistency (simplified)\n",
    "    scores['Consistency'] = consistency_score  # Use previously calculated\n",
    "    \n",
    "    # Uniqueness\n",
    "    scores['Uniqueness'] = uniqueness_score  # Use previously calculated\n",
    "    \n",
    "    # Accuracy\n",
    "    scores['Accuracy'] = accuracy_score  # Use previously calculated\n",
    "    \n",
    "    # Calculate overall score\n",
    "    overall_score = np.mean(list(scores.values()))\n",
    "    \n",
    "    # Print scores\n",
    "    print(\"\\nQuality Dimensions:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def get_grade(score):\n",
    "        if score >= 95: return 'A+ üåü'\n",
    "        elif score >= 90: return 'A'\n",
    "        elif score >= 85: return 'B+'\n",
    "        elif score >= 80: return 'B'\n",
    "        elif score >= 75: return 'C+'\n",
    "        elif score >= 70: return 'C'\n",
    "        elif score >= 65: return 'D'\n",
    "        else: return 'F ‚ö†Ô∏è'\n",
    "    \n",
    "    for dimension, score in scores.items():\n",
    "        bar = '‚ñà' * int(score / 5) + '‚ñë' * (20 - int(score / 5))\n",
    "        print(f\"{dimension:15} {bar} {score:5.1f}% ({get_grade(score)})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\n{'OVERALL SCORE:':15} {'‚ñà' * int(overall_score / 5)}{'‚ñë' * (20 - int(overall_score / 5))} {overall_score:5.1f}% ({get_grade(overall_score)})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    print(\"\\nTop Priority Issues to Address:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    priorities = []\n",
    "    if scores['Completeness'] < 90:\n",
    "        priorities.append(\"1. Fill missing values or remove incomplete records\")\n",
    "    if scores['Validity'] < 90:\n",
    "        priorities.append(\"2. Fix invalid email formats and date values\")\n",
    "    if scores['Consistency'] < 90:\n",
    "        priorities.append(\"3. Standardize text capitalization and date formats\")\n",
    "    if scores['Uniqueness'] < 100:\n",
    "        priorities.append(\"4. Remove or merge duplicate records\")\n",
    "    if scores['Accuracy'] < 90:\n",
    "        priorities.append(\"5. Investigate and handle outliers\")\n",
    "    \n",
    "    for priority in priorities[:3]:  # Show top 3 priorities\n",
    "        print(f\"  {priority}\")\n",
    "    \n",
    "    return overall_score\n",
    "\n",
    "overall_quality_score = generate_quality_report(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Remove temporary files created during the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up files created during the lesson\n",
    "import os\n",
    "\n",
    "files_to_remove = ['sample_data.csv', 'company_data.json', 'company_report.xlsx']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    try:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "            print(f\"Removed: {file}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}