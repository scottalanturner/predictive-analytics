{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2, Lesson 4: Data Integration and Real-World Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Common Integration Challenges\n",
    "**Real Problems You'll Face When Combining Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Mismatched Column Names\n",
    "Different systems often use different names for the same data. This is one of the most common integration problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System A: Sales Database\n",
    "sales_system = pd.DataFrame({\n",
    "    'cust_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'cust_name': ['Alice Johnson', 'Bob Smith', 'Carol White', 'David Lee', 'Eva Brown'],\n",
    "    'purchase_amt': [250.00, 175.50, 320.75, 95.00, 445.25],\n",
    "    'trans_date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19']\n",
    "})\n",
    "\n",
    "# System B: CRM System\n",
    "crm_system = pd.DataFrame({\n",
    "    'CustomerID': [1001, 1002, 1003, 1004, 1005, 1006],\n",
    "    'FullName': ['Alice Johnson', 'Bob Smith', 'Carol White', 'David Lee', 'Eva Brown', 'Frank Wilson'],\n",
    "    'Email': ['alice@email.com', 'bob@email.com', 'carol@email.com', \n",
    "             'david@email.com', 'eva@email.com', 'frank@email.com'],\n",
    "    'CustomerSince': ['2023-01-01', '2023-02-15', '2023-03-20', '2023-04-10', '2023-05-05', '2023-06-01']\n",
    "})\n",
    "\n",
    "print(\"PROBLEM: Different column names for same data\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nSales System columns:\", sales_system.columns.tolist())\n",
    "print(\"CRM System columns:\", crm_system.columns.tolist())\n",
    "print(\"\\nSales System sample:\")\n",
    "print(sales_system.head(3))\n",
    "print(\"\\nCRM System sample:\")\n",
    "print(crm_system.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Solving Column Name Mismatches\n",
    "Create a mapping dictionary to standardize column names before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column mapping\n",
    "sales_rename = {\n",
    "    'cust_id': 'customer_id',\n",
    "    'cust_name': 'customer_name',\n",
    "    'purchase_amt': 'amount',\n",
    "    'trans_date': 'date'\n",
    "}\n",
    "\n",
    "crm_rename = {\n",
    "    'CustomerID': 'customer_id',\n",
    "    'FullName': 'customer_name',\n",
    "    'Email': 'email',\n",
    "    'CustomerSince': 'join_date'\n",
    "}\n",
    "\n",
    "# Standardize column names\n",
    "sales_clean = sales_system.rename(columns=sales_rename)\n",
    "crm_clean = crm_system.rename(columns=crm_rename)\n",
    "\n",
    "print(\"SOLUTION: Standardized column names\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nStandardized Sales columns:\", sales_clean.columns.tolist())\n",
    "print(\"Standardized CRM columns:\", crm_clean.columns.tolist())\n",
    "\n",
    "# Now we can merge!\n",
    "integrated_data = pd.merge(sales_clean, crm_clean[['customer_id', 'email', 'join_date']], \n",
    "                           on='customer_id', how='left')\n",
    "\n",
    "print(\"\\nIntegrated Data:\")\n",
    "print(integrated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Different Data Formats\n",
    "The same information can be stored in completely different formats across systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System A: Dates as strings in different formats\n",
    "system_a = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4, 5],\n",
    "    'date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'],\n",
    "    'amount': ['$1,234.56', '$987.65', '$2,345.67', '$456.78', '$3,456.78']\n",
    "})\n",
    "\n",
    "# System B: Dates in different format, amounts as numbers\n",
    "system_b = pd.DataFrame({\n",
    "    'order_id': [6, 7, 8, 9, 10],\n",
    "    'date': ['01/20/2024', '01/21/2024', '01/22/2024', '01/23/2024', '01/24/2024'],\n",
    "    'amount': [1567.89, 2345.67, 876.54, 3456.78, 1234.56]\n",
    "})\n",
    "\n",
    "print(\"PROBLEM: Different data formats\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nSystem A:\")\n",
    "print(system_a)\n",
    "print(f\"Date type: {system_a['date'].dtype}\")\n",
    "print(f\"Amount type: {system_a['amount'].dtype}\")\n",
    "\n",
    "print(\"\\nSystem B:\")\n",
    "print(system_b)\n",
    "print(f\"Date type: {system_b['date'].dtype}\")\n",
    "print(f\"Amount type: {system_b['amount'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Standardizing Data Formats\n",
    "Convert all data to consistent formats before combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean currency strings\n",
    "def clean_currency(value):\n",
    "    if isinstance(value, str):\n",
    "        return float(value.replace('$', '').replace(',', ''))\n",
    "    return value\n",
    "\n",
    "# Standardize System A\n",
    "system_a_clean = system_a.copy()\n",
    "system_a_clean['date'] = pd.to_datetime(system_a_clean['date'])\n",
    "system_a_clean['amount'] = system_a_clean['amount'].apply(clean_currency)\n",
    "\n",
    "# Standardize System B\n",
    "system_b_clean = system_b.copy()\n",
    "system_b_clean['date'] = pd.to_datetime(system_b_clean['date'])\n",
    "\n",
    "print(\"SOLUTION: Standardized formats\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine the cleaned data\n",
    "combined_orders = pd.concat([system_a_clean, system_b_clean], ignore_index=True)\n",
    "combined_orders = combined_orders.sort_values('date')\n",
    "\n",
    "print(\"\\nCombined and sorted data:\")\n",
    "print(combined_orders)\n",
    "print(f\"\\nAll dates are now: {combined_orders['date'].dtype}\")\n",
    "print(f\"All amounts are now: {combined_orders['amount'].dtype}\")\n",
    "print(f\"\\nTotal revenue: ${combined_orders['amount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Key Matching Problems\n",
    "**When IDs Don't Match Perfectly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Inconsistent ID Formats\n",
    "Different systems might store IDs with different prefixes, padding, or formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database 1: Product IDs with prefix\n",
    "inventory = pd.DataFrame({\n",
    "    'product_id': ['PROD-001', 'PROD-002', 'PROD-003', 'PROD-004', 'PROD-005'],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "    'stock_qty': [15, 50, 30, 8, 25]\n",
    "})\n",
    "\n",
    "# Database 2: Product IDs without prefix, different padding\n",
    "sales = pd.DataFrame({\n",
    "    'prod_code': ['1', '2', '3', '4', '5', '01', '02'],\n",
    "    'units_sold': [3, 10, 5, 2, 8, 2, 5],\n",
    "    'sale_date': pd.date_range('2024-01-15', periods=7)\n",
    "})\n",
    "\n",
    "print(\"PROBLEM: IDs don't match between systems\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nInventory System IDs:\", inventory['product_id'].tolist())\n",
    "print(\"\\nSales System IDs:\", sales['prod_code'].tolist())\n",
    "print(\"\\nDirect merge would fail!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Creating a Mapping Table\n",
    "Sometimes you need to create a lookup table to map between different ID systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID mapping functions\n",
    "def standardize_inventory_id(id_str):\n",
    "    # Extract numeric part from 'PROD-XXX'\n",
    "    return int(id_str.split('-')[1])\n",
    "\n",
    "def standardize_sales_id(id_str):\n",
    "    # Convert to integer, removing any leading zeros\n",
    "    return int(id_str)\n",
    "\n",
    "# Add standardized IDs\n",
    "inventory['std_id'] = inventory['product_id'].apply(standardize_inventory_id)\n",
    "sales['std_id'] = sales['prod_code'].apply(standardize_sales_id)\n",
    "\n",
    "print(\"SOLUTION: Standardized IDs for matching\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nInventory with standard IDs:\")\n",
    "print(inventory[['product_id', 'std_id', 'product_name']])\n",
    "\n",
    "print(\"\\nSales with standard IDs:\")\n",
    "print(sales[['prod_code', 'std_id', 'units_sold']])\n",
    "\n",
    "# Now merge on standardized IDs\n",
    "integrated = pd.merge(sales, inventory[['std_id', 'product_name', 'stock_qty']], \n",
    "                      on='std_id', how='left')\n",
    "\n",
    "print(\"\\nIntegrated data:\")\n",
    "print(integrated[['sale_date', 'product_name', 'units_sold', 'stock_qty']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Handling Missing Data During Integration\n",
    "**What to Do When Data Doesn't Exist in All Sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Different Types of Joins\n",
    "Understanding when to use inner, left, right, and outer joins is crucial for integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer database\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "})\n",
    "\n",
    "# Orders database (not all customers have orders)\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104],\n",
    "    'customer_id': [1, 2, 1, 6],  # Customer 6 doesn't exist!\n",
    "    'amount': [100, 200, 150, 300]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCustomers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)\n",
    "\n",
    "# Demonstrate different join types\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"JOIN TYPE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Inner join - only matching records\n",
    "inner = pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "print(f\"\\nINNER JOIN (only customers with orders): {len(inner)} rows\")\n",
    "print(inner)\n",
    "\n",
    "# Left join - all customers\n",
    "left = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "print(f\"\\nLEFT JOIN (all customers): {len(left)} rows\")\n",
    "print(left)\n",
    "\n",
    "# Right join - all orders\n",
    "right = pd.merge(customers, orders, on='customer_id', how='right')\n",
    "print(f\"\\nRIGHT JOIN (all orders): {len(right)} rows\")\n",
    "print(right)\n",
    "\n",
    "# Outer join - everything\n",
    "outer = pd.merge(customers, orders, on='customer_id', how='outer')\n",
    "print(f\"\\nOUTER JOIN (all records): {len(outer)} rows\")\n",
    "print(outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: Handling Missing Values After Joins\n",
    "After joining, you often need to deal with NaN values that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the left join result from above\n",
    "merged_data = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "\n",
    "print(\"Data after LEFT JOIN:\")\n",
    "print(merged_data)\n",
    "print(\"\\nMissing values:\")\n",
    "print(merged_data.isna().sum())\n",
    "\n",
    "# Strategy 1: Fill with defaults\n",
    "merged_clean1 = merged_data.copy()\n",
    "merged_clean1['order_id'] = merged_clean1['order_id'].fillna(0).astype(int)\n",
    "merged_clean1['amount'] = merged_clean1['amount'].fillna(0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Strategy 1: Fill with defaults (0)\")\n",
    "print(merged_clean1)\n",
    "\n",
    "# Strategy 2: Add indicator column\n",
    "merged_clean2 = merged_data.copy()\n",
    "merged_clean2['has_order'] = ~merged_clean2['order_id'].isna()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Strategy 2: Add indicator column\")\n",
    "print(merged_clean2)\n",
    "\n",
    "# Strategy 3: Aggregate to customer level\n",
    "customer_summary = merged_data.groupby(['customer_id', 'name', 'city']).agg({\n",
    "    'order_id': 'count',\n",
    "    'amount': 'sum'\n",
    "}).reset_index()\n",
    "customer_summary.columns = ['customer_id', 'name', 'city', 'order_count', 'total_spent']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Strategy 3: Aggregate to customer level\")\n",
    "print(customer_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Time-Based Integration\n",
    "**Aligning Data from Different Time Periods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9: Different Granularities\n",
    "One system might have daily data while another has monthly summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sales data\n",
    "daily_sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', '2024-01-31'),\n",
    "    'daily_revenue': np.random.uniform(1000, 3000, 31).round(2)\n",
    "})\n",
    "\n",
    "# Weekly marketing spend\n",
    "weekly_marketing = pd.DataFrame({\n",
    "    'week_start': pd.date_range('2024-01-01', '2024-01-29', freq='W-MON'),\n",
    "    'marketing_spend': [5000, 4500, 5500, 4800, 5200]\n",
    "})\n",
    "\n",
    "# Monthly targets\n",
    "monthly_targets = pd.DataFrame({\n",
    "    'month': ['2024-01'],\n",
    "    'target_revenue': [60000],\n",
    "    'target_customers': [500]\n",
    "})\n",
    "\n",
    "print(\"PROBLEM: Different time granularities\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nDaily data: {len(daily_sales)} records\")\n",
    "print(daily_sales.head())\n",
    "print(f\"\\nWeekly data: {len(weekly_marketing)} records\")\n",
    "print(weekly_marketing)\n",
    "print(f\"\\nMonthly data: {len(monthly_targets)} records\")\n",
    "print(monthly_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10: Aligning Different Time Granularities\n",
    "Convert everything to a common granularity or use appropriate aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add week and month to daily data\n",
    "daily_sales['week'] = daily_sales['date'].dt.to_period('W').dt.start_time\n",
    "daily_sales['month'] = daily_sales['date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate daily to weekly\n",
    "weekly_sales = daily_sales.groupby('week')['daily_revenue'].sum().reset_index()\n",
    "weekly_sales.columns = ['week_start', 'weekly_revenue']\n",
    "\n",
    "# Merge weekly data\n",
    "weekly_combined = pd.merge(weekly_sales, weekly_marketing, on='week_start', how='outer')\n",
    "\n",
    "print(\"SOLUTION: Aligned weekly data\")\n",
    "print(\"=\" * 50)\n",
    "print(weekly_combined)\n",
    "\n",
    "# Calculate weekly ROI\n",
    "weekly_combined['roi'] = (weekly_combined['weekly_revenue'] / weekly_combined['marketing_spend']).round(2)\n",
    "print(\"\\nWeekly ROI Analysis:\")\n",
    "print(weekly_combined[['week_start', 'roi']])\n",
    "\n",
    "# Monthly summary\n",
    "monthly_actual = daily_sales.groupby('month')['daily_revenue'].sum().reset_index()\n",
    "monthly_actual.columns = ['month', 'actual_revenue']\n",
    "monthly_actual['month'] = monthly_actual['month'].astype(str)\n",
    "\n",
    "monthly_comparison = pd.merge(monthly_targets, monthly_actual, on='month')\n",
    "monthly_comparison['pct_of_target'] = (monthly_comparison['actual_revenue'] / \n",
    "                                       monthly_comparison['target_revenue'] * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Monthly Performance vs Target:\")\n",
    "print(monthly_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-World Integration Scenarios\n",
    "**Complete Examples from Different Industries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11: E-Commerce Data Integration\n",
    "Combining website analytics, order system, and inventory to get a complete picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website Analytics\n",
    "web_analytics = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-15', periods=7),\n",
    "    'sessions': [1200, 1350, 1100, 1500, 1650, 1400, 1250],\n",
    "    'unique_visitors': [800, 900, 750, 1000, 1100, 950, 850],\n",
    "    'page_views': [3600, 4050, 3300, 4500, 4950, 4200, 3750],\n",
    "    'bounce_rate': [0.35, 0.32, 0.38, 0.30, 0.28, 0.33, 0.36]\n",
    "})\n",
    "\n",
    "# Order System\n",
    "orders = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-15', periods=7),\n",
    "    'orders_placed': [45, 52, 38, 61, 67, 55, 48],\n",
    "    'revenue': [4500, 5200, 3800, 6100, 6700, 5500, 4800],\n",
    "    'avg_order_value': [100, 100, 100, 100, 100, 100, 100],\n",
    "    'cancelled_orders': [2, 3, 1, 4, 3, 2, 2]\n",
    "})\n",
    "\n",
    "# Inventory System\n",
    "inventory = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-15', periods=7),\n",
    "    'products_in_stock': [150, 145, 142, 138, 131, 126, 122],\n",
    "    'out_of_stock_items': [5, 6, 7, 8, 10, 11, 12],\n",
    "    'restock_orders': [0, 0, 1, 0, 0, 1, 0]\n",
    "})\n",
    "\n",
    "print(\"E-COMMERCE DATA INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Integrate all three sources\n",
    "ecommerce_dashboard = web_analytics.merge(orders, on='date').merge(inventory, on='date')\n",
    "\n",
    "# Calculate key metrics\n",
    "ecommerce_dashboard['conversion_rate'] = (ecommerce_dashboard['orders_placed'] / \n",
    "                                          ecommerce_dashboard['unique_visitors'] * 100).round(2)\n",
    "ecommerce_dashboard['fulfillment_rate'] = ((ecommerce_dashboard['orders_placed'] - \n",
    "                                           ecommerce_dashboard['cancelled_orders']) / \n",
    "                                          ecommerce_dashboard['orders_placed'] * 100).round(2)\n",
    "ecommerce_dashboard['stock_coverage'] = (ecommerce_dashboard['products_in_stock'] / \n",
    "                                         ecommerce_dashboard['orders_placed']).round(1)\n",
    "\n",
    "# Display integrated dashboard\n",
    "print(\"\\nIntegrated E-Commerce Dashboard:\")\n",
    "display_cols = ['date', 'unique_visitors', 'orders_placed', 'revenue', \n",
    "                'conversion_rate', 'fulfillment_rate', 'stock_coverage']\n",
    "print(ecommerce_dashboard[display_cols])\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WEEKLY SUMMARY:\")\n",
    "print(f\"Total Visitors: {ecommerce_dashboard['unique_visitors'].sum():,}\")\n",
    "print(f\"Total Orders: {ecommerce_dashboard['orders_placed'].sum()}\")\n",
    "print(f\"Total Revenue: ${ecommerce_dashboard['revenue'].sum():,.2f}\")\n",
    "print(f\"Avg Conversion Rate: {ecommerce_dashboard['conversion_rate'].mean():.2f}%\")\n",
    "print(f\"Avg Fulfillment Rate: {ecommerce_dashboard['fulfillment_rate'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 12: Healthcare Data Integration\n",
    "Combining patient records, lab results, and appointment data while maintaining privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient Records (de-identified)\n",
    "patients = pd.DataFrame({\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'age_group': ['30-40', '50-60', '40-50', '60-70', '20-30'],\n",
    "    'gender': ['F', 'M', 'F', 'M', 'F'],\n",
    "    'risk_category': ['Low', 'High', 'Medium', 'High', 'Low'],\n",
    "    'enrollment_date': pd.to_datetime(['2023-01-15', '2023-02-01', '2023-01-20', '2023-03-01', '2023-02-15'])\n",
    "})\n",
    "\n",
    "# Lab Results\n",
    "lab_results = pd.DataFrame({\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005', 'P001', 'P002'],\n",
    "    'test_date': pd.to_datetime(['2024-01-10', '2024-01-11', '2024-01-12', '2024-01-13', '2024-01-14',\n",
    "                                 '2024-01-15', '2024-01-16']),\n",
    "    'test_type': ['Blood', 'Blood', 'Urine', 'Blood', 'Blood', 'Urine', 'Blood'],\n",
    "    'result_flag': ['Normal', 'Abnormal', 'Normal', 'Abnormal', 'Normal', 'Normal', 'Critical']\n",
    "})\n",
    "\n",
    "# Appointments\n",
    "appointments = pd.DataFrame({\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005', 'P001'],\n",
    "    'appointment_date': pd.to_datetime(['2024-01-20', '2024-01-21', '2024-01-22', '2024-01-23', \n",
    "                                        '2024-01-24', '2024-01-25']),\n",
    "    'appointment_type': ['Follow-up', 'Emergency', 'Routine', 'Follow-up', 'Routine', 'Routine'],\n",
    "    'attended': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes']\n",
    "})\n",
    "\n",
    "print(\"HEALTHCARE DATA INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Note: All patient data is de-identified\")\n",
    "print(\"\\nData Sources:\")\n",
    "print(f\"- Patient Records: {len(patients)} patients\")\n",
    "print(f\"- Lab Results: {len(lab_results)} tests\")\n",
    "print(f\"- Appointments: {len(appointments)} appointments\")\n",
    "\n",
    "# Create patient summary\n",
    "patient_summary = patients.copy()\n",
    "\n",
    "# Add lab result summary\n",
    "lab_summary = lab_results.groupby('patient_id').agg({\n",
    "    'test_date': 'count',\n",
    "    'result_flag': lambda x: (x != 'Normal').sum()\n",
    "}).reset_index()\n",
    "lab_summary.columns = ['patient_id', 'total_tests', 'abnormal_results']\n",
    "\n",
    "# Add appointment summary\n",
    "appt_summary = appointments.groupby('patient_id').agg({\n",
    "    'appointment_date': 'count',\n",
    "    'attended': lambda x: (x == 'No').sum()\n",
    "}).reset_index()\n",
    "appt_summary.columns = ['patient_id', 'total_appointments', 'missed_appointments']\n",
    "\n",
    "# Integrate all data\n",
    "integrated_patient_data = patient_summary.merge(lab_summary, on='patient_id', how='left')\\\n",
    "                                         .merge(appt_summary, on='patient_id', how='left')\n",
    "\n",
    "# Fill NaN with 0 for patients with no tests/appointments\n",
    "integrated_patient_data = integrated_patient_data.fillna(0)\n",
    "\n",
    "# Calculate risk scores\n",
    "def calculate_risk_score(row):\n",
    "    score = 0\n",
    "    if row['risk_category'] == 'High': score += 3\n",
    "    elif row['risk_category'] == 'Medium': score += 2\n",
    "    else: score += 1\n",
    "    \n",
    "    score += row['abnormal_results'] * 2\n",
    "    score += row['missed_appointments'] * 1.5\n",
    "    \n",
    "    return score\n",
    "\n",
    "integrated_patient_data['risk_score'] = integrated_patient_data.apply(calculate_risk_score, axis=1)\n",
    "\n",
    "print(\"\\nIntegrated Patient Dashboard:\")\n",
    "print(integrated_patient_data)\n",
    "\n",
    "# Priority patients\n",
    "high_priority = integrated_patient_data[integrated_patient_data['risk_score'] > 5]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGH PRIORITY PATIENTS (Risk Score > 5):\")\n",
    "print(high_priority[['patient_id', 'risk_category', 'abnormal_results', 'missed_appointments', 'risk_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 13: Financial Data Integration\n",
    "Combining transaction data, account information, and market data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account Information\n",
    "accounts = pd.DataFrame({\n",
    "    'account_id': ['ACC001', 'ACC002', 'ACC003', 'ACC004', 'ACC005'],\n",
    "    'account_type': ['Checking', 'Savings', 'Investment', 'Checking', 'Savings'],\n",
    "    'opening_balance': [5000, 10000, 25000, 3000, 15000],\n",
    "    'account_status': ['Active', 'Active', 'Active', 'Dormant', 'Active']\n",
    "})\n",
    "\n",
    "# Daily Transactions\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1001, 1016),\n",
    "    'account_id': ['ACC001', 'ACC002', 'ACC001', 'ACC003', 'ACC005',\n",
    "                  'ACC001', 'ACC002', 'ACC003', 'ACC001', 'ACC005',\n",
    "                  'ACC002', 'ACC003', 'ACC001', 'ACC003', 'ACC005'],\n",
    "    'transaction_date': pd.date_range('2024-01-15', periods=15),\n",
    "    'type': ['Deposit', 'Withdrawal', 'Withdrawal', 'Deposit', 'Deposit',\n",
    "            'Withdrawal', 'Deposit', 'Withdrawal', 'Deposit', 'Withdrawal',\n",
    "            'Withdrawal', 'Deposit', 'Withdrawal', 'Deposit', 'Deposit'],\n",
    "    'amount': [500, 200, 100, 5000, 1000,\n",
    "              300, 150, 2000, 400, 500,\n",
    "              100, 3000, 250, 1500, 2000]\n",
    "})\n",
    "\n",
    "# Market Rates (daily)\n",
    "market_rates = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-15', periods=15),\n",
    "    'savings_rate': [0.04, 0.04, 0.041, 0.041, 0.042, 0.042, 0.042, 0.043, 0.043, 0.043,\n",
    "                    0.044, 0.044, 0.044, 0.045, 0.045],\n",
    "    'investment_return': [0.002, -0.001, 0.003, 0.004, -0.002, 0.001, 0.003, -0.001, 0.002, 0.001,\n",
    "                         0.003, 0.002, -0.001, 0.004, 0.002]\n",
    "})\n",
    "\n",
    "print(\"FINANCIAL DATA INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate daily balances\n",
    "daily_activity = transactions.groupby(['account_id', 'transaction_date', 'type'])['amount'].sum().reset_index()\n",
    "daily_pivot = daily_activity.pivot_table(index=['account_id', 'transaction_date'], \n",
    "                                         columns='type', values='amount', fill_value=0).reset_index()\n",
    "\n",
    "# Calculate net daily change\n",
    "daily_pivot['net_change'] = daily_pivot.get('Deposit', 0) - daily_pivot.get('Withdrawal', 0)\n",
    "\n",
    "# Merge with account information\n",
    "account_activity = pd.merge(accounts, daily_pivot, on='account_id', how='left')\n",
    "\n",
    "# Calculate running balance\n",
    "account_activity = account_activity.sort_values(['account_id', 'transaction_date'])\n",
    "account_activity['running_balance'] = account_activity.groupby('account_id')['net_change'].cumsum() + \\\n",
    "                                      account_activity['opening_balance']\n",
    "\n",
    "print(\"\\nAccount Activity with Running Balances:\")\n",
    "print(account_activity[['account_id', 'transaction_date', 'Deposit', 'Withdrawal', \n",
    "                        'net_change', 'running_balance']].head(10))\n",
    "\n",
    "# Account Summary\n",
    "account_summary = transactions.groupby('account_id').agg({\n",
    "    'transaction_id': 'count',\n",
    "    'amount': ['sum', 'mean']\n",
    "}).reset_index()\n",
    "account_summary.columns = ['account_id', 'total_transactions', 'total_volume', 'avg_transaction']\n",
    "\n",
    "final_summary = pd.merge(accounts, account_summary, on='account_id', how='left')\n",
    "final_summary = final_summary.fillna(0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ACCOUNT SUMMARY:\")\n",
    "print(final_summary)\n",
    "\n",
    "# Risk Assessment\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RISK ASSESSMENT:\")\n",
    "dormant_accounts = final_summary[final_summary['account_status'] == 'Dormant']\n",
    "if len(dormant_accounts) > 0:\n",
    "    print(f\"Dormant accounts found: {dormant_accounts['account_id'].tolist()}\")\n",
    "\n",
    "high_activity = final_summary[final_summary['total_transactions'] > 5]\n",
    "print(f\"High activity accounts (>5 transactions): {high_activity['account_id'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Data Lineage and Documentation\n",
    "**Tracking Where Data Comes From**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 14: Creating a Data Lineage Tracker\n",
    "Document transformations and source systems for audit and debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_data_lineage(df, source_system, transformations):\n",
    "    \"\"\"\n",
    "    Add metadata columns to track data lineage.\n",
    "    \"\"\"\n",
    "    df_tracked = df.copy()\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df_tracked['_source_system'] = source_system\n",
    "    df_tracked['_load_timestamp'] = pd.Timestamp.now()\n",
    "    df_tracked['_transformations'] = ', '.join(transformations)\n",
    "    df_tracked['_version'] = '1.0'\n",
    "    \n",
    "    return df_tracked\n",
    "\n",
    "# Example: Track lineage for integrated data\n",
    "raw_sales = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C'],\n",
    "    'sales': ['100', '200', '150']  # Note: stored as strings\n",
    "})\n",
    "\n",
    "# Apply transformations and track them\n",
    "transformations_applied = []\n",
    "\n",
    "# Transformation 1: Convert sales to numeric\n",
    "processed_sales = raw_sales.copy()\n",
    "processed_sales['sales'] = pd.to_numeric(processed_sales['sales'])\n",
    "transformations_applied.append('Convert sales to numeric')\n",
    "\n",
    "# Transformation 2: Add calculated field\n",
    "processed_sales['sales_with_tax'] = processed_sales['sales'] * 1.08\n",
    "transformations_applied.append('Add 8% tax calculation')\n",
    "\n",
    "# Transformation 3: Add category\n",
    "processed_sales['category'] = 'Electronics'\n",
    "transformations_applied.append('Add category field')\n",
    "\n",
    "# Add lineage tracking\n",
    "final_data = track_data_lineage(processed_sales, 'SALES_SYSTEM_V2', transformations_applied)\n",
    "\n",
    "print(\"DATA WITH LINEAGE TRACKING:\")\n",
    "print(\"=\" * 60)\n",
    "print(final_data)\n",
    "\n",
    "print(\"\\nLineage Metadata:\")\n",
    "print(\"-\" * 40)\n",
    "for col in final_data.columns:\n",
    "    if col.startswith('_'):\n",
    "        print(f\"{col}: {final_data[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 15: Integration Pipeline Summary\n",
    "Create a summary report of all data sources and integration steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_integration_report(integration_steps):\n",
    "    \"\"\"\n",
    "    Generate a report documenting the integration process.\n",
    "    \"\"\"\n",
    "    report = pd.DataFrame(integration_steps)\n",
    "    return report\n",
    "\n",
    "# Document a complete integration pipeline\n",
    "integration_pipeline = [\n",
    "    {\n",
    "        'step': 1,\n",
    "        'name': 'Extract Sales Data',\n",
    "        'source': 'SQL Database',\n",
    "        'records_in': 10000,\n",
    "        'records_out': 10000,\n",
    "        'duration_sec': 2.3,\n",
    "        'status': 'Success'\n",
    "    },\n",
    "    {\n",
    "        'step': 2,\n",
    "        'name': 'Extract Customer Data',\n",
    "        'source': 'CRM API',\n",
    "        'records_in': 5000,\n",
    "        'records_out': 5000,\n",
    "        'duration_sec': 5.1,\n",
    "        'status': 'Success'\n",
    "    },\n",
    "    {\n",
    "        'step': 3,\n",
    "        'name': 'Clean Sales Data',\n",
    "        'source': 'Memory',\n",
    "        'records_in': 10000,\n",
    "        'records_out': 9850,\n",
    "        'duration_sec': 1.2,\n",
    "        'status': 'Success (150 records removed)'\n",
    "    },\n",
    "    {\n",
    "        'step': 4,\n",
    "        'name': 'Standardize IDs',\n",
    "        'source': 'Memory',\n",
    "        'records_in': 14850,\n",
    "        'records_out': 14850,\n",
    "        'duration_sec': 0.8,\n",
    "        'status': 'Success'\n",
    "    },\n",
    "    {\n",
    "        'step': 5,\n",
    "        'name': 'Merge Datasets',\n",
    "        'source': 'Memory',\n",
    "        'records_in': 14850,\n",
    "        'records_out': 9500,\n",
    "        'duration_sec': 1.5,\n",
    "        'status': 'Success (inner join)'\n",
    "    },\n",
    "    {\n",
    "        'step': 6,\n",
    "        'name': 'Calculate Metrics',\n",
    "        'source': 'Memory',\n",
    "        'records_in': 9500,\n",
    "        'records_out': 9500,\n",
    "        'duration_sec': 0.5,\n",
    "        'status': 'Success'\n",
    "    },\n",
    "    {\n",
    "        'step': 7,\n",
    "        'name': 'Export Results',\n",
    "        'source': 'Memory',\n",
    "        'records_in': 9500,\n",
    "        'records_out': 9500,\n",
    "        'duration_sec': 3.2,\n",
    "        'status': 'Success (saved to warehouse)'\n",
    "    }\n",
    "]\n",
    "\n",
    "pipeline_report = create_integration_report(integration_pipeline)\n",
    "\n",
    "print(\"INTEGRATION PIPELINE REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Pipeline Execution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"-\" * 70)\n",
    "print(pipeline_report.to_string(index=False))\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Steps: {len(pipeline_report)}\")\n",
    "print(f\"Total Duration: {pipeline_report['duration_sec'].sum():.1f} seconds\")\n",
    "print(f\"Records Processed: {pipeline_report['records_in'].iloc[0]:,} → {pipeline_report['records_out'].iloc[-1]:,}\")\n",
    "print(f\"Data Reduction: {(1 - pipeline_report['records_out'].iloc[-1] / pipeline_report['records_in'].iloc[0]) * 100:.1f}%\")\n",
    "print(f\"All Steps: {'✓ Success' if all(pipeline_report['status'].str.contains('Success')) else '✗ Errors Detected'}\")\n",
    "\n",
    "# Data quality metrics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA QUALITY METRICS:\")\n",
    "print(\"-\" * 40)\n",
    "quality_metrics = {\n",
    "    'Completeness': '98.5%',\n",
    "    'Accuracy': '99.2%',\n",
    "    'Consistency': '97.8%',\n",
    "    'Timeliness': 'Real-time (< 15 sec delay)',\n",
    "    'Validity': '99.5%',\n",
    "    'Uniqueness': '100% (no duplicates)'\n",
    "}\n",
    "\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"{metric:15} {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}