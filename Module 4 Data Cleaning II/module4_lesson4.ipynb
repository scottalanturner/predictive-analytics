{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Lesson 4: Building Production Pipelines\n",
    "\n",
    "## From Notebook to Production: Professional Workflows\n",
    "\n",
    "### What You'll Learn:\n",
    "- Building end-to-end scikit-learn pipelines\n",
    "- Creating custom transformers for your specific needs\n",
    "- Using ColumnTransformer for mixed data types\n",
    "- Preventing data leakage with proper pipeline design\n",
    "- Cross-validation with pipelines\n",
    "- Saving and deploying pipelines\n",
    "- Performance optimization techniques\n",
    "- Production best practices\n",
    "\n",
    "### Why This Matters:\n",
    "Moving from experimental notebooks to production systems requires reproducible, maintainable, and error-free code. Pipelines ensure that every transformation is applied consistently, in the right order, without data leakage. This is the difference between a proof-of-concept and a real-world ML system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "Let's import everything we'll need for building production pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Scikit-learn pipeline tools ready for production!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Scikit-learn pipeline tools ready for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Pipeline Architecture\n",
    "\n",
    "### What is a Pipeline?\n",
    "\n",
    "A pipeline chains together multiple data transformation steps and a final estimator (model). Each step is applied in sequence, and the entire pipeline can be treated as a single object.\n",
    "\n",
    "**Benefits:**\n",
    "- **Reproducibility**: Same transformations applied consistently\n",
    "- **No data leakage**: Transformations fitted only on training data\n",
    "- **Convenience**: Single object to fit, predict, and deploy\n",
    "- **Maintainability**: Clean, organized code\n",
    "\n",
    "Let's start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset created:\n",
      "Shape: (1000, 20)\n",
      "Target distribution: {0: 502, 1: 498}\n",
      "\n",
      "First few rows:\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -4.906442   3.442789   0.558964  -0.976764  -1.568805  -4.271982   \n",
      "1   2.162610  -5.286651   2.609846  -1.803898  -1.831216   1.450757   \n",
      "2  -4.784844  -3.744827   4.657592  -1.408806  -5.444758  -2.416013   \n",
      "3  10.465024   1.070944  -3.562432  -0.849062   2.183860  -0.609893   \n",
      "4   5.599516  -1.776412  -1.304322  -0.720074   5.859373  -3.292432   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
      "0  -3.727921   0.111868   2.119795  -2.522812    3.352281   -7.492478   \n",
      "1   2.648709   2.152307   0.524552   0.493548   -1.401809    6.680603   \n",
      "2   3.556495  -1.572119  -0.730549   3.447661   -2.609052    7.961059   \n",
      "3   0.946327  -1.046141  -2.057053  -2.056650   -2.215455   -1.449095   \n",
      "4   3.152205   7.099882  -3.321076   3.245486   -0.336178    6.608729   \n",
      "\n",
      "   feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
      "0    4.264669    0.304866    0.777693   -9.375464    1.654446    3.012859   \n",
      "1   -2.431830    2.462773   -1.254824    2.978402   -3.428457   -4.562178   \n",
      "2   -5.151105    0.473131   -4.070667   -0.932309   -3.230768   -7.844646   \n",
      "3   -1.217685    2.026805    2.121829    3.184256   -1.960146    0.782147   \n",
      "4    5.632297   -1.943748    1.169455    3.782513   -4.752822   -7.577624   \n",
      "\n",
      "   feature_18  feature_19  \n",
      "0   -4.497003   -2.520066  \n",
      "1    3.698665   -1.923286  \n",
      "2    2.803798   -2.963189  \n",
      "3   -1.444202    0.915985  \n",
      "4    4.868025    1.708210  \n"
     ]
    }
   ],
   "source": [
    "# Create sample dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for clarity\n",
    "feature_names = [f'feature_{i}' for i in range(20)]\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(\"Sample dataset created:\")\n",
    "print(f\"Shape: {X_df.shape}\")\n",
    "print(f\"Target distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Your First Pipeline\n",
    "\n",
    "Let's build a simple pipeline that scales features and applies a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Pipeline Results:\n",
      "Accuracy: 0.900\n",
      "\n",
      "‚úÖ The pipeline:\n",
      "1. Scales features (fit on train, transform both)\n",
      "2. Trains classifier on scaled data\n",
      "3. Makes predictions on scaled test data\n",
      "All in one fit() and predict() call!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Method 1: Using Pipeline class\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit entire pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Simple Pipeline Results:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"\\n‚úÖ The pipeline:\")\n",
    "print(\"1. Scales features (fit on train, transform both)\")\n",
    "print(\"2. Trains classifier on scaled data\")\n",
    "print(\"3. Makes predictions on scaled test data\")\n",
    "print(\"All in one fit() and predict() call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using make_pipeline\n",
    "\n",
    "For simpler syntax, use `make_pipeline` which automatically names the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make_pipeline Results:\n",
      "Accuracy: 0.900\n",
      "\n",
      "Pipeline steps (auto-named):\n",
      "  standardscaler: StandardScaler\n",
      "  randomforestclassifier: RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Method 2: Using make_pipeline (automatic naming)\n",
    "simple_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit and evaluate\n",
    "simple_pipeline.fit(X_train, y_train)\n",
    "accuracy = simple_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Make_pipeline Results:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"\\nPipeline steps (auto-named):\")\n",
    "for name, step in simple_pipeline.named_steps.items():\n",
    "    print(f\"  {name}: {step.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Mixed Data Types with ColumnTransformer\n",
    "\n",
    "### The Real-World Challenge\n",
    "\n",
    "Real datasets have mixed types: numeric features need scaling, categorical features need encoding, and some features might need custom transformations. `ColumnTransformer` handles this elegantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed-type dataset:\n",
      "         age        income  credit_score    education     employment  \\\n",
      "0  37.476086  26309.911254    834.342980     Bachelor      Full-time   \n",
      "1  67.437144   5794.448838    482.240854  High School      Full-time   \n",
      "2  56.063685  32215.334697    565.122582          PhD  Self-employed   \n",
      "3  49.130241  40561.951268    407.853742     Bachelor      Full-time   \n",
      "4  26.112969  38553.048228    635.929039       Master  Self-employed   \n",
      "\n",
      "       city has_mortgage  approved  \n",
      "0   Houston          Yes         1  \n",
      "1   Chicago           No         1  \n",
      "2   Houston          Yes         0  \n",
      "3   Phoenix           No         1  \n",
      "4  New York          Yes         1  \n",
      "\n",
      "Data types:\n",
      "age             float64\n",
      "income          float64\n",
      "credit_score    float64\n",
      "education        object\n",
      "employment       object\n",
      "city             object\n",
      "has_mortgage     object\n",
      "approved          int64\n",
      "dtype: object\n",
      "\n",
      "üìù We need different preprocessing for each type!\n"
     ]
    }
   ],
   "source": [
    "# Create a realistic dataset with mixed types\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    # Numeric features\n",
    "    'age': np.random.uniform(18, 70, n_samples),\n",
    "    'income': np.random.lognormal(10, 1, n_samples),\n",
    "    'credit_score': np.random.uniform(300, 850, n_samples),\n",
    "    \n",
    "    # Categorical features\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
    "    'employment': np.random.choice(['Full-time', 'Part-time', 'Self-employed'], n_samples),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
    "    \n",
    "    # Binary feature\n",
    "    'has_mortgage': np.random.choice(['Yes', 'No'], n_samples),\n",
    "    \n",
    "    # Target\n",
    "    'approved': np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n",
    "})\n",
    "\n",
    "print(\"Mixed-type dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nüìù We need different preprocessing for each type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a ColumnTransformer\n",
    "\n",
    "Let's create a preprocessing pipeline that handles each data type appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Pipeline with ColumnTransformer:\n",
      "Accuracy: 0.655\n",
      "\n",
      "‚úÖ Each column type processed appropriately:\n",
      "  - Numeric: Imputed (median) ‚Üí Scaled\n",
      "  - Categorical: Imputed (constant) ‚Üí One-hot encoded\n",
      "  - Binary: Imputed (mode) ‚Üí Binary encoded\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Identify column types\n",
    "numeric_features = ['age', 'income', 'credit_score']\n",
    "categorical_features = ['education', 'employment', 'city']\n",
    "binary_features = ['has_mortgage']\n",
    "\n",
    "# Create preprocessing pipelines for each type\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "binary_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine into ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('bin', binary_pipeline, binary_features)\n",
    "    ])\n",
    "\n",
    "# Create full pipeline with model\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('approved', axis=1)\n",
    "y = df['approved']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and evaluate\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "accuracy = full_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Full Pipeline with ColumnTransformer:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"\\n‚úÖ Each column type processed appropriately:\")\n",
    "print(\"  - Numeric: Imputed (median) ‚Üí Scaled\")\n",
    "print(\"  - Categorical: Imputed (constant) ‚Üí One-hot encoded\")\n",
    "print(\"  - Binary: Imputed (mode) ‚Üí Binary encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Custom Transformers\n",
    "\n",
    "### When Built-in Transformers Aren't Enough\n",
    "\n",
    "Sometimes you need custom transformations. Here's how to create transformers that integrate seamlessly with scikit-learn pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing custom transformers:\n",
      "\n",
      "Original income stats:\n",
      "count      1000.000000\n",
      "mean      39250.174494\n",
      "std       46037.236602\n",
      "min        1186.365269\n",
      "25%       12529.975968\n",
      "50%       23961.790988\n",
      "75%       46149.098972\n",
      "max      536653.314326\n",
      "Name: income, dtype: float64\n",
      "\n",
      "After log transformation:\n",
      "count    1000.000000\n",
      "mean       10.098963\n",
      "std         0.988865\n",
      "min         7.079492\n",
      "25%         9.435959\n",
      "50%        10.084256\n",
      "75%        10.739653\n",
      "max        13.193109\n",
      "Name: income, dtype: float64\n",
      "\n",
      "After outlier capping (5th-95th percentile):\n",
      "count      1000.000000\n",
      "mean      35978.686333\n",
      "std       32650.627898\n",
      "min        4836.202374\n",
      "25%       12529.975968\n",
      "50%       23961.790988\n",
      "75%       46149.098972\n",
      "max      123870.609145\n",
      "Name: income, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply log transformation to specified columns\"\"\"\n",
    "    \n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Nothing to learn\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if input is a DataFrame or numpy array\n",
    "        if hasattr(X, 'columns'):\n",
    "            # It's a DataFrame\n",
    "            X_copy = X.copy()\n",
    "            \n",
    "            if self.columns:\n",
    "                for col in self.columns:\n",
    "                    if col in X_copy.columns:\n",
    "                        X_copy[col] = np.log1p(X_copy[col])  # log1p handles zeros\n",
    "            else:\n",
    "                # Apply to all numeric columns\n",
    "                numeric_cols = X_copy.select_dtypes(include=[np.number]).columns\n",
    "                X_copy[numeric_cols] = np.log1p(X_copy[numeric_cols])\n",
    "        else:\n",
    "            # It's a numpy array\n",
    "            X_copy = X.copy() if hasattr(X, 'copy') else np.array(X)\n",
    "            \n",
    "            if self.columns is not None:\n",
    "                # If columns specified as indices\n",
    "                if isinstance(self.columns, (list, tuple)):\n",
    "                    for col_idx in self.columns:\n",
    "                        if isinstance(col_idx, int) and col_idx < X_copy.shape[1]:\n",
    "                            X_copy[:, col_idx] = np.log1p(X_copy[:, col_idx])\n",
    "            else:\n",
    "                # Apply to all columns\n",
    "                X_copy = np.log1p(X_copy)\n",
    "            \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Cap outliers at specified percentiles\"\"\"\n",
    "    \n",
    "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.bounds_ = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Check if input is a DataFrame or numpy array\n",
    "        if hasattr(X, 'columns'):\n",
    "            # It's a DataFrame\n",
    "            numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                self.bounds_[col] = {\n",
    "                    'lower': np.percentile(X[col], self.lower_percentile),\n",
    "                    'upper': np.percentile(X[col], self.upper_percentile)\n",
    "                }\n",
    "        else:\n",
    "            # It's a numpy array\n",
    "            n_features = X.shape[1] if len(X.shape) > 1 else 1\n",
    "            \n",
    "            for i in range(n_features):\n",
    "                if len(X.shape) > 1:\n",
    "                    col_data = X[:, i]\n",
    "                else:\n",
    "                    col_data = X\n",
    "                    \n",
    "                self.bounds_[i] = {\n",
    "                    'lower': np.percentile(col_data, self.lower_percentile),\n",
    "                    'upper': np.percentile(col_data, self.upper_percentile)\n",
    "                }\n",
    "                \n",
    "                if len(X.shape) == 1:\n",
    "                    break\n",
    "                    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if input is a DataFrame or numpy array\n",
    "        if hasattr(X, 'columns'):\n",
    "            # It's a DataFrame\n",
    "            X_copy = X.copy()\n",
    "            \n",
    "            for col, bounds in self.bounds_.items():\n",
    "                if col in X_copy.columns:\n",
    "                    X_copy[col] = X_copy[col].clip(\n",
    "                        lower=bounds['lower'],\n",
    "                        upper=bounds['upper']\n",
    "                    )\n",
    "        else:\n",
    "            # It's a numpy array\n",
    "            X_copy = X.copy() if hasattr(X, 'copy') else np.array(X)\n",
    "            \n",
    "            for col_idx, bounds in self.bounds_.items():\n",
    "                if isinstance(col_idx, int):\n",
    "                    if len(X_copy.shape) > 1 and col_idx < X_copy.shape[1]:\n",
    "                        X_copy[:, col_idx] = np.clip(\n",
    "                            X_copy[:, col_idx],\n",
    "                            bounds['lower'],\n",
    "                            bounds['upper']\n",
    "                        )\n",
    "                    elif len(X_copy.shape) == 1:\n",
    "                        X_copy = np.clip(X_copy, bounds['lower'], bounds['upper'])\n",
    "                        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# Test custom transformers\n",
    "print(\"Testing custom transformers:\")\n",
    "print(\"\\nOriginal income stats:\")\n",
    "print(df['income'].describe())\n",
    "\n",
    "# Apply log transformation\n",
    "log_transformer = LogTransformer(columns=['income'])\n",
    "df_log = log_transformer.fit_transform(df)\n",
    "print(\"\\nAfter log transformation:\")\n",
    "print(df_log['income'].describe())\n",
    "\n",
    "# Apply outlier capping\n",
    "outlier_capper = OutlierCapper(lower_percentile=5, upper_percentile=95)\n",
    "df_capped = outlier_capper.fit_transform(df)\n",
    "print(\"\\nAfter outlier capping (5th-95th percentile):\")\n",
    "print(df_capped['income'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Custom Transformer: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: 8\n",
      "After feature engineering: 12\n",
      "\n",
      "New features created:\n",
      "  - income_per_year\n",
      "  - credit_age_interaction\n",
      "  - credit_income_ratio\n",
      "  - age_income_interaction\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create interaction and polynomial features\"\"\"\n",
    "    \n",
    "    def __init__(self, create_interactions=True, create_ratios=True, \n",
    "                 age_col=None, income_col=None, credit_col=None):\n",
    "        self.create_interactions = create_interactions\n",
    "        self.create_ratios = create_ratios\n",
    "        # For DataFrame: column names, for array: column indices\n",
    "        self.age_col = age_col\n",
    "        self.income_col = income_col\n",
    "        self.credit_col = credit_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Auto-detect column positions if not specified\n",
    "        if hasattr(X, 'columns'):\n",
    "            # It's a DataFrame - find column positions\n",
    "            if self.age_col is None and 'age' in X.columns:\n",
    "                self.age_col = 'age'\n",
    "            if self.income_col is None and 'income' in X.columns:\n",
    "                self.income_col = 'income'\n",
    "            if self.credit_col is None and 'credit_score' in X.columns:\n",
    "                self.credit_col = 'credit_score'\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if hasattr(X, 'columns'):\n",
    "            # It's a DataFrame\n",
    "            X_copy = X.copy()\n",
    "            \n",
    "            if self.create_interactions:\n",
    "                # Age-income interaction\n",
    "                if self.age_col in X_copy.columns and self.income_col in X_copy.columns:\n",
    "                    X_copy['age_income_interaction'] = X_copy[self.age_col] * X_copy[self.income_col]\n",
    "                \n",
    "                # Credit score age interaction\n",
    "                if self.credit_col in X_copy.columns and self.age_col in X_copy.columns:\n",
    "                    X_copy['credit_age_interaction'] = X_copy[self.credit_col] * X_copy[self.age_col]\n",
    "            \n",
    "            if self.create_ratios:\n",
    "                # Income to age ratio\n",
    "                if self.income_col in X_copy.columns and self.age_col in X_copy.columns:\n",
    "                    X_copy['income_per_year'] = X_copy[self.income_col] / (X_copy[self.age_col] + 1)\n",
    "                \n",
    "                # Credit to income ratio\n",
    "                if self.credit_col in X_copy.columns and self.income_col in X_copy.columns:\n",
    "                    X_copy['credit_income_ratio'] = X_copy[self.credit_col] / (X_copy[self.income_col] + 1)\n",
    "            \n",
    "            return X_copy\n",
    "        else:\n",
    "            # It's a numpy array - we need to work with indices\n",
    "            X_copy = X.copy() if hasattr(X, 'copy') else np.array(X)\n",
    "            new_features = []\n",
    "            \n",
    "            # Map column identifiers to indices if they're integers\n",
    "            age_idx = self.age_col if isinstance(self.age_col, int) else None\n",
    "            income_idx = self.income_col if isinstance(self.income_col, int) else None\n",
    "            credit_idx = self.credit_col if isinstance(self.credit_col, int) else None\n",
    "            \n",
    "            # For production pipeline, we know the order: age=0, income=1, credit_score=2\n",
    "            # This is based on the numeric_features list order\n",
    "            if age_idx is None and X.shape[1] >= 3:\n",
    "                age_idx = 0\n",
    "            if income_idx is None and X.shape[1] >= 3:\n",
    "                income_idx = 1\n",
    "            if credit_idx is None and X.shape[1] >= 3:\n",
    "                credit_idx = 2\n",
    "            \n",
    "            if self.create_interactions:\n",
    "                # Age-income interaction\n",
    "                if age_idx is not None and income_idx is not None and \\\n",
    "                   age_idx < X_copy.shape[1] and income_idx < X_copy.shape[1]:\n",
    "                    new_features.append(X_copy[:, age_idx] * X_copy[:, income_idx])\n",
    "                \n",
    "                # Credit score age interaction\n",
    "                if credit_idx is not None and age_idx is not None and \\\n",
    "                   credit_idx < X_copy.shape[1] and age_idx < X_copy.shape[1]:\n",
    "                    new_features.append(X_copy[:, credit_idx] * X_copy[:, age_idx])\n",
    "            \n",
    "            if self.create_ratios:\n",
    "                # Income to age ratio\n",
    "                if income_idx is not None and age_idx is not None and \\\n",
    "                   income_idx < X_copy.shape[1] and age_idx < X_copy.shape[1]:\n",
    "                    new_features.append(X_copy[:, income_idx] / (X_copy[:, age_idx] + 1))\n",
    "                \n",
    "                # Credit to income ratio\n",
    "                if credit_idx is not None and income_idx is not None and \\\n",
    "                   credit_idx < X_copy.shape[1] and income_idx < X_copy.shape[1]:\n",
    "                    new_features.append(X_copy[:, credit_idx] / (X_copy[:, income_idx] + 1))\n",
    "            \n",
    "            # Concatenate new features if any were created\n",
    "            if new_features:\n",
    "                new_features_array = np.column_stack(new_features)\n",
    "                X_copy = np.hstack([X_copy, new_features_array])\n",
    "            \n",
    "            return X_copy\n",
    "\n",
    "\n",
    "# Test feature engineering\n",
    "feature_engineer = FeatureEngineer(age_col='age', income_col='income', credit_col='credit_score')\n",
    "df_engineered = feature_engineer.fit_transform(df)\n",
    "\n",
    "print(\"Original columns:\", df.shape[1])\n",
    "print(\"After feature engineering:\", df_engineered.shape[1])\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = set(df_engineered.columns) - set(df.columns)\n",
    "for feature in new_features:\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Production Pipeline\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "Let's build a complete, production-ready pipeline that includes:\n",
    "1. Custom feature engineering\n",
    "2. Outlier handling\n",
    "3. Missing value imputation\n",
    "4. Scaling and encoding\n",
    "5. Feature selection\n",
    "6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production dataset with missing values:\n",
      "age              0\n",
      "income          50\n",
      "credit_score     0\n",
      "education       30\n",
      "employment       0\n",
      "city             0\n",
      "has_mortgage     0\n",
      "approved         0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "COMPLETE PRODUCTION PIPELINE RESULTS\n",
      "==================================================\n",
      "Training accuracy: 1.000\n",
      "Testing accuracy: 0.645\n",
      "\n",
      "‚úÖ Pipeline steps executed:\n",
      "1. feature_engineer: FeatureEngineer\n",
      "2. preprocessor: ColumnTransformer\n",
      "3. feature_selection: SelectKBest\n",
      "4. classifier: RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "# Add some missing values to make it realistic\n",
    "df_prod = df.copy()\n",
    "missing_indices = np.random.choice(df_prod.index, size=50, replace=False)\n",
    "df_prod.loc[missing_indices, 'income'] = np.nan\n",
    "missing_indices = np.random.choice(df_prod.index, size=30, replace=False)\n",
    "df_prod.loc[missing_indices, 'education'] = np.nan\n",
    "\n",
    "print(\"Production dataset with missing values:\")\n",
    "print(df_prod.isnull().sum())\n",
    "\n",
    "# Complete production pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Create the feature engineering step with column specifications\n",
    "feature_engineer = FeatureEngineer(age_col='age', income_col='income', credit_col='credit_score')\n",
    "\n",
    "# Apply feature engineering to get column names\n",
    "df_temp = feature_engineer.fit_transform(df_prod.drop('approved', axis=1))\n",
    "all_numeric = df_temp.select_dtypes(include=[np.number]).columns.tolist()\n",
    "all_categorical = df_temp.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Numeric preprocessing\n",
    "# Note: LogTransformer with columns=[1] because 'income' is the second column in numeric_features\n",
    "# numeric_features = ['age', 'income', 'credit_score'] so income is at index 1\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outlier_capper', OutlierCapper(lower_percentile=5, upper_percentile=95)),\n",
    "    ('log_transformer', LogTransformer(columns=[1])),  # Apply log to income column (index 1)\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical preprocessing\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing with dynamic column selection\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, all_numeric),\n",
    "        ('cat', categorical_transformer, all_categorical)\n",
    "    ])\n",
    "\n",
    "# Complete pipeline\n",
    "production_pipeline = Pipeline([\n",
    "    ('feature_engineer', FeatureEngineer(age_col='age', income_col='income', credit_col='credit_score')),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=20)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X_prod = df_prod.drop('approved', axis=1)\n",
    "y_prod = df_prod['approved']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prod, y_prod, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit pipeline\n",
    "production_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = production_pipeline.score(X_train, y_train)\n",
    "test_score = production_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPLETE PRODUCTION PIPELINE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training accuracy: {train_score:.3f}\")\n",
    "print(f\"Testing accuracy: {test_score:.3f}\")\n",
    "print(\"\\n‚úÖ Pipeline steps executed:\")\n",
    "for i, (name, step) in enumerate(production_pipeline.named_steps.items(), 1):\n",
    "    print(f\"{i}. {name}: {step.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation with Pipelines\n",
    "\n",
    "### Preventing Data Leakage\n",
    "\n",
    "One of the biggest advantages of pipelines is that they prevent data leakage during cross-validation. The preprocessing steps are fitted only on the training folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation with Pipeline (No Data Leakage):\n",
      "CV Scores: [0.665 0.65  0.675 0.685 0.675]\n",
      "Mean CV Score: 0.670 (+/- 0.024)\n",
      "\n",
      "‚úÖ Why this prevents data leakage:\n",
      "1. For each fold, the pipeline is fitted ONLY on training data\n",
      "2. Scaler means/stds calculated from training fold only\n",
      "3. Encoder categories learned from training fold only\n",
      "4. Test fold transformed using training parameters\n",
      "5. This mimics real-world deployment perfectly!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Cross-validation with pipeline (CORRECT - no data leakage)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Note: This might take a moment to run\n",
    "cv_scores = cross_val_score(\n",
    "    production_pipeline, \n",
    "    X_prod, \n",
    "    y_prod, \n",
    "    cv=cv, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Cross-validation with Pipeline (No Data Leakage):\")\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Why this prevents data leakage:\")\n",
    "print(\"1. For each fold, the pipeline is fitted ONLY on training data\")\n",
    "print(\"2. Scaler means/stds calculated from training fold only\")\n",
    "print(\"3. Encoder categories learned from training fold only\")\n",
    "print(\"4. Test fold transformed using training parameters\")\n",
    "print(\"5. This mimics real-world deployment perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and Loading Pipelines\n",
    "\n",
    "### Deployment Ready\n",
    "\n",
    "Once your pipeline is trained, you need to save it for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved with joblib\n",
      "Pipeline saved with pickle\n",
      "\n",
      "File sizes:\n",
      "Joblib: 3041.5 KB\n",
      "Pickle: 3031.4 KB\n",
      "\n",
      "üí° Joblib is usually more efficient for NumPy arrays\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Method 1: Using joblib (recommended for scikit-learn)\n",
    "joblib.dump(production_pipeline, 'production_pipeline.joblib')\n",
    "print(\"Pipeline saved with joblib\")\n",
    "\n",
    "# Method 2: Using pickle\n",
    "with open('production_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(production_pipeline, f)\n",
    "print(\"Pipeline saved with pickle\")\n",
    "\n",
    "# Check file sizes\n",
    "joblib_size = os.path.getsize('production_pipeline.joblib') / 1024  # KB\n",
    "pickle_size = os.path.getsize('production_pipeline.pkl') / 1024  # KB\n",
    "\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"Joblib: {joblib_size:.1f} KB\")\n",
    "print(f\"Pickle: {pickle_size:.1f} KB\")\n",
    "print(\"\\nüí° Joblib is usually more efficient for NumPy arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Using in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data for prediction:\n",
      "   age   income  credit_score education     employment         city  \\\n",
      "0   25  45000.0           720  Bachelor      Full-time      Chicago   \n",
      "1   45  85000.0           650       PhD  Self-employed     New York   \n",
      "2   33      NaN           800    Master      Full-time  Los Angeles   \n",
      "\n",
      "  has_mortgage  \n",
      "0           No  \n",
      "1          Yes  \n",
      "2          Yes  \n",
      "\n",
      "Predictions:\n",
      "Application 1: Rejected (Probability: 47.00%)\n",
      "Application 2: Approved (Probability: 65.00%)\n",
      "Application 3: Rejected (Probability: 36.00%)\n",
      "\n",
      "‚úÖ Note: The pipeline handled the missing value automatically!\n"
     ]
    }
   ],
   "source": [
    "# Simulate production environment\n",
    "# Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('production_pipeline.joblib')\n",
    "\n",
    "# Simulate new data coming in\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [25, 45, 33],\n",
    "    'income': [45000, 85000, np.nan],  # Note: missing value!\n",
    "    'credit_score': [720, 650, 800],\n",
    "    'education': ['Bachelor', 'PhD', 'Master'],\n",
    "    'employment': ['Full-time', 'Self-employed', 'Full-time'],\n",
    "    'city': ['Chicago', 'New York', 'Los Angeles'],\n",
    "    'has_mortgage': ['No', 'Yes', 'Yes']\n",
    "})\n",
    "\n",
    "print(\"New data for prediction:\")\n",
    "print(new_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_pipeline.predict(new_data)\n",
    "probabilities = loaded_pipeline.predict_proba(new_data)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for i, (pred, prob) in enumerate(zip(predictions, probabilities[:, 1])):\n",
    "    status = \"Approved\" if pred == 1 else \"Rejected\"\n",
    "    print(f\"Application {i+1}: {status} (Probability: {prob:.2%})\")\n",
    "\n",
    "print(\"\\n‚úÖ Note: The pipeline handled the missing value automatically!\")\n",
    "\n",
    "# Clean up\n",
    "os.remove('production_pipeline.joblib')\n",
    "os.remove('production_pipeline.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
